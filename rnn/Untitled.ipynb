{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.l1 = nn.RNN(256,40,1, batch_first=True)\n",
    "        self.l2 = nn.Linear(40, 256)\n",
    "\n",
    "    def forward(self, input):\n",
    "        _, l1o = self.l1(input)\n",
    "        output = self.l2(l1o)\n",
    "        return output\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwik9 = open('enwik9', 'rb').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 10, device=device)\n",
    "y = torch.randn(1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20, 100):#len(enwik9)\n",
    "    text_in = enwik9[i-20:i]\n",
    "    char_out = enwik9[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byte_to_tensor(c):\n",
    "    tensor = torch.zeros(256)\n",
    "    tensor[c] = 1\n",
    "    return tensor\n",
    "\n",
    "def bytes_to_tensor(b):\n",
    "    tensor = torch.zeros(len(b), 256)\n",
    "    for idx, c in enumerate(b):\n",
    "        tensor[idx][c] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1594,  0.2814, -0.2901,  0.0052,  0.0497,  0.2934,  0.1070,\n",
       "            0.1368,  0.1765, -0.1226,  0.1894, -0.0215, -0.0437,  0.0323,\n",
       "            0.2266, -0.0776,  0.1209, -0.0064,  0.0033, -0.2500, -0.0315,\n",
       "            0.1000,  0.1812,  0.1018, -0.0332,  0.3113, -0.2247,  0.1814,\n",
       "           -0.2251, -0.0967,  0.2895, -0.1670, -0.1737,  0.1420, -0.1413,\n",
       "            0.0784, -0.1128, -0.1471, -0.1236, -0.2602],\n",
       "          [ 0.0547,  0.3182, -0.2519,  0.0331, -0.2086,  0.3373,  0.2168,\n",
       "           -0.0199,  0.1638, -0.0727, -0.0028, -0.0366, -0.0833,  0.0534,\n",
       "            0.0896,  0.2121,  0.4529,  0.0595, -0.1920, -0.2421,  0.1143,\n",
       "            0.1770,  0.1872,  0.0940, -0.0650,  0.1360, -0.0988,  0.0857,\n",
       "           -0.0363, -0.1746,  0.2555, -0.1206,  0.3261,  0.2186, -0.0048,\n",
       "            0.1349,  0.0097, -0.3780, -0.2607,  0.0974],\n",
       "          [-0.1595,  0.1698, -0.1605,  0.1026, -0.0610,  0.3749,  0.0339,\n",
       "            0.1156,  0.0995,  0.1777,  0.1848,  0.1159, -0.2118, -0.0933,\n",
       "           -0.0562,  0.1912,  0.4655, -0.0908, -0.2658, -0.2370,  0.0047,\n",
       "           -0.0666,  0.1987,  0.1929,  0.0416,  0.0105, -0.3759,  0.2077,\n",
       "           -0.0287, -0.3692,  0.2943, -0.2187,  0.1283,  0.1655, -0.0381,\n",
       "           -0.0372, -0.0176, -0.4777, -0.1655, -0.0629],\n",
       "          [ 0.0589,  0.2902, -0.2721,  0.0393,  0.1470,  0.2914,  0.2170,\n",
       "            0.2161,  0.1642,  0.0985,  0.2489,  0.2273, -0.0790, -0.1423,\n",
       "           -0.0569, -0.1614,  0.2382,  0.2482, -0.1481, -0.3509, -0.0815,\n",
       "            0.2743,  0.2292,  0.0827, -0.3033,  0.1629, -0.2961,  0.2164,\n",
       "           -0.1840, -0.0906,  0.4263, -0.2546, -0.0267,  0.3432, -0.1586,\n",
       "            0.0142, -0.1112, -0.2761, -0.1507, -0.2030],\n",
       "          [ 0.0750,  0.2325, -0.2521,  0.0063, -0.1124,  0.3896,  0.1753,\n",
       "           -0.1205,  0.2102,  0.0431, -0.0442, -0.0442,  0.0828,  0.0396,\n",
       "            0.0404,  0.1156,  0.4545,  0.0186, -0.1863, -0.2866,  0.1232,\n",
       "            0.2051,  0.2007,  0.1101, -0.1008, -0.0092, -0.1638,  0.1052,\n",
       "           -0.0317, -0.1523,  0.1990, -0.2619,  0.4183,  0.2651, -0.0512,\n",
       "            0.0435, -0.0604, -0.3819, -0.2375,  0.0991],\n",
       "          [-0.1024,  0.1737, -0.1950,  0.0209, -0.0528,  0.3732,  0.0084,\n",
       "            0.0666,  0.0717,  0.1715,  0.1539,  0.1494, -0.1402, -0.0580,\n",
       "           -0.1654,  0.2192,  0.4756, -0.0429, -0.2152, -0.2031, -0.0134,\n",
       "           -0.0790,  0.1926,  0.1941,  0.0429, -0.0197, -0.3455,  0.2208,\n",
       "           -0.0238, -0.3477,  0.3368, -0.2549,  0.1085,  0.2145, -0.0892,\n",
       "           -0.0311, -0.0544, -0.4272, -0.2114, -0.0620],\n",
       "          [ 0.0336,  0.3040, -0.2433,  0.0226,  0.1397,  0.2814,  0.1923,\n",
       "            0.1937,  0.2019,  0.1144,  0.2351,  0.1830, -0.0835, -0.1341,\n",
       "           -0.0773, -0.1250,  0.1761,  0.2101, -0.1629, -0.3443, -0.0606,\n",
       "            0.2265,  0.2351,  0.0707, -0.3222,  0.1510, -0.3416,  0.2347,\n",
       "           -0.2224, -0.0801,  0.4240, -0.2883, -0.0215,  0.3270, -0.1255,\n",
       "            0.0364, -0.0879, -0.2557, -0.1269, -0.1909],\n",
       "          [ 0.1024,  0.2468, -0.2404,  0.0219, -0.1059,  0.4029,  0.1805,\n",
       "           -0.1128,  0.1895,  0.0209, -0.0540, -0.0280,  0.0838,  0.0471,\n",
       "            0.0506,  0.1501,  0.4596,  0.0239, -0.1762, -0.2734,  0.1224,\n",
       "            0.2050,  0.1851,  0.1140, -0.1063,  0.0236, -0.1338,  0.1275,\n",
       "           -0.0108, -0.1543,  0.2104, -0.2668,  0.4037,  0.2617, -0.0638,\n",
       "            0.0505, -0.0670, -0.3534, -0.2634,  0.1105],\n",
       "          [-0.1146,  0.1819, -0.1906,  0.0296, -0.0639,  0.3686,  0.0128,\n",
       "            0.0634,  0.0710,  0.1733,  0.1610,  0.1529, -0.1549, -0.0655,\n",
       "           -0.1619,  0.2275,  0.4645, -0.0508, -0.2268, -0.2148, -0.0079,\n",
       "           -0.0911,  0.1830,  0.1771,  0.0412, -0.0244, -0.3610,  0.2183,\n",
       "           -0.0286, -0.3526,  0.3439, -0.2590,  0.1138,  0.1991, -0.0784,\n",
       "           -0.0270, -0.0344, -0.4279, -0.1912, -0.0585],\n",
       "          [ 0.0389,  0.3082, -0.2434,  0.0328,  0.1487,  0.2837,  0.1922,\n",
       "            0.1977,  0.1961,  0.1139,  0.2416,  0.1882, -0.0853, -0.1350,\n",
       "           -0.0777, -0.1257,  0.1883,  0.2152, -0.1586, -0.3415, -0.0627,\n",
       "            0.2281,  0.2315,  0.0759, -0.3170,  0.1580, -0.3317,  0.2317,\n",
       "           -0.2181, -0.0804,  0.4249, -0.2851, -0.0287,  0.3202, -0.1318,\n",
       "            0.0338, -0.0925, -0.2497, -0.1318, -0.1865]]],\n",
       "        grad_fn=<TransposeBackward1>),\n",
       " tensor([[[ 0.0389,  0.3082, -0.2434,  0.0328,  0.1487,  0.2837,  0.1922,\n",
       "            0.1977,  0.1961,  0.1139,  0.2416,  0.1882, -0.0853, -0.1350,\n",
       "           -0.0777, -0.1257,  0.1883,  0.2152, -0.1586, -0.3415, -0.0627,\n",
       "            0.2281,  0.2315,  0.0759, -0.3170,  0.1580, -0.3317,  0.2317,\n",
       "           -0.2181, -0.0804,  0.4249, -0.2851, -0.0287,  0.3202, -0.1318,\n",
       "            0.0338, -0.0925, -0.2497, -0.1318, -0.1865]]],\n",
       "        grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = bytes_to_tensor(b'abcabcabca')\n",
    "x = torch.reshape(t, (1,10,256))\n",
    "model.l1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99980 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-5202079f1f78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss.backward()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Does the update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ml/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for i in range(20):\n",
    "    total_loss = 0\n",
    "    for i in tqdm(range(20, 100000)):#len(enwik9)\n",
    "        optimizer.zero_grad()\n",
    "        text_in = enwik9[i-10:i]\n",
    "        char_out = enwik9[i+1]\n",
    "        t_x = bytes_to_tensor(text_in)\n",
    "        t_x = torch.reshape(t_x, (1,10,256))\n",
    "        t_y = byte_to_tensor(char_out)\n",
    "        y = model(t_x)\n",
    "        y = torch.reshape(y, (1,256))\n",
    "        loss = criterion(y, torch.tensor([char_out]))\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "    print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(99)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(torch.reshape(bytes_to_tensor(b'this is a message. whe'[-10:]), (1,10,256))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
